name: python-test
on: #[push,workflow_dispatch] # added multiple triggers , workflow_dispatch -- manual trigger

# Using multiple events
# If you specify multiple events, only one of those events needs to occur to trigger your workflow. 
# If multiple triggering events for your workflow occur at the same time, multiple workflow runs will be triggered.
  pull_request: 
    types: closed
  push:
    branches:
      - main  
      - 'mona/octocat'
      - 'releases/**'
          # Sequence of patterns matched against refs/tags
    tags:        
      - v2
      - v1.*
jobs:
  python_code_test:
    runs-on: ubuntu-latest
    outputs:  
      model_pkl_file: ${{ steps.pkl_file_name.outputs.pkl_file }} # step context information , which get the step output using the step id mentioned 
      # contexts : https://docs.github.com/en/actions/learn-github-actions/contexts
      # can specify more than on output of steps
    steps:
      - name: Get_code
        uses: actions/checkout@v3 
      - name: get python 3.8
        uses: actions/setup-python@v4
        with:
            python-version: '3.9' # this will set up python 3.9,3.10.., as default 
            #cache: 'pip' # caching pip dependencies is built in actions/setup-python@v4 
            #cache dependency : https://docs.github.com/en/actions/using-workflows/caching-dependencies-to-speed-up-workflows
      - name: cache dependecies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: requirments-${{ hashFiles('**/requirements.txt') }}
      - name: install dependencies
        run: |
          pip install -r requirements.txt
          python --version
      - name: run the code 
        run: python iris_classification.py
      - name: Get name of the experiment folder name 
        id: pkl_file_name                        # id for step to use in contexts, should be used after step of creation of the files that is finding
        run: find mlruns/*/*/artifacts/svc_test/*.pkl -type f -execdir echo 'pkl_file={}' >>$GITHUB_OUTPUT ';' # finding a pkl file in the path and storing it in $GITHUB_OUTPUT variable  
      - name: get files and folder
        uses: actions/upload-artifact@v3 # action to upload the folder into github
        with:
          name: my-artifact # name of the artifact to upload which will be used for download as well
          path: mlruns  # path of the file or folder to upload
          # artifacts : https://docs.github.com/en/actions/using-workflows/storing-workflow-data-as-artifacts
  build_docker:
    needs: python_code_test  # will run only after successful completion of python_code_test job , should used as start of the job 
    runs-on:  ubuntu-latest  # every job has its own runner and there are isolated from each other
    steps:                   # jobs will run in parallel by default
      - name: Get_code
        uses: actions/checkout@v3
      - name: get python 3.8
        uses: actions/setup-python@v4
        with:
            python-version: '3.9' # this will set up python 3,.9 as default 
            #cache: 'pip' # caching pip dependencies
      - name: cache dependecies
        uses: actions/cache@v3
        with:
            path: ~/.cache/pip
            key: requirments-${{ hashFiles('**/requirements.txt') }}
      - name: install dependencies
        run: |
          pip install -r requirements.txt
          python --version 
      - name: build docker iamge
        run: echo "build docker image" 
      - name: download the artifact from github runner
        uses: actions/download-artifact@v3 # to download the artifact created from github runner , it will unzip the contents in the zip folder and place it in same path where runner is executing 
        with : 
          name: my-artifact # name of the artifact created ,and it has to be a sequential job
      - name : get contents # t osee the contents of the artifacts folder alnong with other files
        run: ls 
      - name : get the output of the previous step
        run: echo "${{needs.python_code_test.outputs.model_pkl_file}}" # needs ,the dependency job which holds all the outputs of the dependency job 
          
